{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "094aa01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "import re\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d65619f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Syllable_dictionary.txt\") as f:\n",
    "    syllables = [line.rstrip('\\n') for line in f]\n",
    "    f.close()\n",
    "with open(\"shakespeare.txt\") as f:\n",
    "#     data = [line.rstrip('\\n') for line in f]\n",
    "    dataChar = [line for line in f]\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d485b02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "syllableDict = {}\n",
    "for n in range(len(syllables)):\n",
    "    splitted = syllables[n].split()\n",
    "    syllableDict[splitted[0]] = splitted[1:]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d05d00be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_144857/2370446308.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data2Lines = []\n",
    "count = 0\n",
    "while True:\n",
    "    if count == len(data):\n",
    "        break\n",
    "    if len(data[count]) == 1:\n",
    "        count += 1\n",
    "        continue\n",
    "    data2Lines.append(data[count].split()+ data[count+1].split())\n",
    "    count += 2\n",
    "\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "43e92ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 460.,    0.,    0.,    0.,    2.,   95.,  790., 1014.,  240.,\n",
       "          14.]),\n",
       " array([ 1. ,  6.9, 12.8, 18.7, 24.6, 30.5, 36.4, 42.3, 48.2, 54.1, 60. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(x) for x in dataChar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4d6f51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"shakespeare.txt\") as f:\n",
    "#     data = [line.rstrip('\\n') for line in f]\n",
    "    dataChar = [line for line in f]\n",
    "    f.close()\n",
    "\n",
    "dataChar = [re.sub('^ *|[1-9]', '', x).lower() for x in dataChar]\n",
    "\n",
    "dataCharStr = ''\n",
    "for n in dataChar:\n",
    "    dataCharStr += n + ' '\n",
    "\n",
    "possibleLetters = ''\n",
    "for n in pd.Series(list(dataCharStr)).unique():\n",
    "    possibleLetters += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9c2f0830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_vectorizer(strng, alphabet=possibleLetters):\n",
    "    vector = [[0 if char != letter else 1 for char in alphabet] \n",
    "                  for letter in strng]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "28a343e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = string_vectorizer(dataCharStr)\n",
    "data40grams = []\n",
    "for n in range(len(vectorized)-40):\n",
    "    data40grams.append(vectorized[n:n+40])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a615de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_char = len(possibleLetters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "fccd91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embed(x)\n",
    "        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n",
    "        out = self.fc(out.reshape(out.shape[0], -1))\n",
    "        return out, (hidden, cell)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "0f5a1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self):\n",
    "        self.chunk_len = 40\n",
    "        self.epoch = 10000\n",
    "        self.batch_size = 1\n",
    "        self.print_every = 100\n",
    "        self.hidden_size = 128\n",
    "        self.num_layers = 1\n",
    "        self.lr = 0.001\n",
    "        \n",
    "    def char_tensor(self, string):\n",
    "        tensor = torch.zeros(len(string)).long()\n",
    "        for c in range(len(string)):\n",
    "            tensor[c] = possibleLetters.index(string[c])\n",
    "        return tensor\n",
    "    \n",
    "    def get_random_batch(self):\n",
    "        start_idx = random.randint(0, len(dataCharStr)-self.chunk_len)\n",
    "        end_idx = start_idx + self.chunk_len + 1\n",
    "        text_str = dataCharStr[start_idx: end_idx]\n",
    "        text_input = torch.zeros(self.batch_size, self.chunk_len)\n",
    "        text_target = torch.zeros(self.batch_size, self.chunk_len)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            text_input[i,:] = self.char_tensor(text_str[:-1])\n",
    "            text_target[i,:] = self.char_tensor(text_str[1:])\n",
    "            \n",
    "        return text_input.long(), text_target.long()\n",
    "\n",
    "    def get_seq_batch(self, index):\n",
    "        start_idx = index\n",
    "        end_idx = start_idx + self.chunk_len + 1\n",
    "        text_str = dataCharStr[start_idx: end_idx]\n",
    "        text_input = torch.zeros(self.batch_size, self.chunk_len)\n",
    "        text_target = torch.zeros(self.batch_size, self.chunk_len)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            text_input[i,:] = self.char_tensor(text_str[:-1])\n",
    "            text_target[i,:] = self.char_tensor(text_str[1:])\n",
    "\n",
    "        return text_input.long(), text_target.long()\n",
    "\n",
    "    \n",
    "    # 560 pred length\n",
    "    def generate(self, initial_str = \"shall i compare thee to a summer's day?\\n\", predict_len = 560, temperature = 0.5):\n",
    "        hidden, cell = self.rnn.init_hidden(batch_size = self.batch_size)\n",
    "        initial_input = self.char_tensor(initial_str)\n",
    "        predicted = initial_str\n",
    "        \n",
    "        for p in range(len(initial_str) - 1):\n",
    "            _, (hidden, cell) = self.rnn(initial_input[p].view(1).to(device), hidden, cell)\n",
    "            \n",
    "        last_char = initial_input[-1]\n",
    "        \n",
    "        for p in range(predict_len):\n",
    "            output, (hidden, cell) = self.rnn(last_char.view(1).to(device), hidden, cell)\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_char = torch.multinomial(output_dist, 1)[0]\n",
    "            predicted_char = possibleLetters[top_char]\n",
    "            predicted += predicted_char\n",
    "            last_char = self.char_tensor(predicted_char)\n",
    "            \n",
    "        return predicted\n",
    "            \n",
    "            \n",
    "            \n",
    "    def train(self):\n",
    "        self.rnn = RNN(n_char, self.hidden_size, self.num_layers, n_char).to(device)\n",
    "        optimizer = optim.Adam(self.rnn.parameters(), lr = self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        print(\"starting Training\")\n",
    "        for epoch in range(self.epoch + 1):\n",
    "#             if epoch > len(dataCharStr)-self.chunk_len:\n",
    "#                 inp, target = self.get_random_batch()\n",
    "#             else:\n",
    "#                 inp, target = self.get_seq_batch(epoch)\n",
    "                \n",
    "            \n",
    "            inp, target = self.get_random_batch()\n",
    "            hidden, cell = self.rnn.init_hidden(batch_size = self.batch_size)\n",
    "            \n",
    "            self.rnn.zero_grad()\n",
    "            loss = 0\n",
    "            inp = inp.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            for c in range(self.chunk_len):\n",
    "                output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n",
    "                loss += criterion(output, target[:,c])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item()/self.chunk_len\n",
    "            if epoch % self.print_every == 0:\n",
    "                print('Epoch '+ str(epoch) + ', Loss: ' + str(loss))\n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "6b3b93c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Training\n",
      "Epoch 0, Loss: 3.6834205627441405\n",
      "Epoch 100, Loss: 2.4205188751220703\n",
      "Epoch 200, Loss: 2.2011014938354494\n",
      "Epoch 300, Loss: 2.207248878479004\n",
      "Epoch 400, Loss: 1.6391851425170898\n",
      "Epoch 500, Loss: 2.055007743835449\n",
      "Epoch 600, Loss: 2.109011650085449\n",
      "Epoch 700, Loss: 1.94483642578125\n",
      "Epoch 800, Loss: 1.8964420318603517\n",
      "Epoch 900, Loss: 1.9573982238769532\n",
      "Epoch 1000, Loss: 2.192524528503418\n",
      "Epoch 1100, Loss: 1.8854570388793945\n",
      "Epoch 1200, Loss: 1.4781264305114745\n",
      "Epoch 1300, Loss: 1.7501296997070312\n",
      "Epoch 1400, Loss: 1.9073600769042969\n",
      "Epoch 1500, Loss: 1.677298355102539\n",
      "Epoch 1600, Loss: 1.614409828186035\n",
      "Epoch 1700, Loss: 1.9906970977783203\n",
      "Epoch 1800, Loss: 1.534061813354492\n",
      "Epoch 1900, Loss: 1.5593320846557617\n",
      "Epoch 2000, Loss: 1.932499885559082\n",
      "Epoch 2100, Loss: 1.664933204650879\n",
      "Epoch 2200, Loss: 1.8069414138793944\n",
      "Epoch 2300, Loss: 1.8845943450927733\n",
      "Epoch 2400, Loss: 1.6798721313476563\n",
      "Epoch 2500, Loss: 2.263457679748535\n",
      "Epoch 2600, Loss: 1.895555305480957\n",
      "Epoch 2700, Loss: 2.3535943984985352\n",
      "Epoch 2800, Loss: 1.6595729827880858\n",
      "Epoch 2900, Loss: 1.8084177017211913\n",
      "Epoch 3000, Loss: 1.667215347290039\n",
      "Epoch 3100, Loss: 1.5452710151672364\n",
      "Epoch 3200, Loss: 1.8881296157836913\n",
      "Epoch 3300, Loss: 1.5317926406860352\n",
      "Epoch 3400, Loss: 1.5586641311645508\n",
      "Epoch 3500, Loss: 1.51389741897583\n",
      "Epoch 3600, Loss: 2.1400390625\n",
      "Epoch 3700, Loss: 1.6018896102905273\n",
      "Epoch 3800, Loss: 1.4730599403381348\n",
      "Epoch 3900, Loss: 1.4806905746459962\n",
      "Epoch 4000, Loss: 1.7143051147460937\n",
      "Epoch 4100, Loss: 1.420107364654541\n",
      "Epoch 4200, Loss: 1.7211994171142577\n",
      "Epoch 4300, Loss: 1.4608007431030274\n",
      "Epoch 4400, Loss: 1.5567848205566406\n",
      "Epoch 4500, Loss: 1.9307966232299805\n",
      "Epoch 4600, Loss: 1.634639358520508\n",
      "Epoch 4700, Loss: 1.6986572265625\n",
      "Epoch 4800, Loss: 1.5332571029663087\n",
      "Epoch 4900, Loss: 1.7495878219604493\n",
      "Epoch 5000, Loss: 1.2753153800964356\n",
      "Epoch 5100, Loss: 1.3026750564575196\n",
      "Epoch 5200, Loss: 1.860792350769043\n",
      "Epoch 5300, Loss: 1.7824529647827148\n",
      "Epoch 5400, Loss: 1.5247625350952148\n",
      "Epoch 5500, Loss: 1.6796586990356446\n",
      "Epoch 5600, Loss: 1.6636482238769532\n",
      "Epoch 5700, Loss: 1.3644370079040526\n",
      "Epoch 5800, Loss: 1.8945672988891602\n",
      "Epoch 5900, Loss: 1.315915584564209\n",
      "Epoch 6000, Loss: 1.4671435356140137\n",
      "Epoch 6100, Loss: 2.2053300857543947\n",
      "Epoch 6200, Loss: 1.5271036148071289\n",
      "Epoch 6300, Loss: 1.4821420669555665\n",
      "Epoch 6400, Loss: 1.5967130661010742\n",
      "Epoch 6500, Loss: 1.2982308387756347\n",
      "Epoch 6600, Loss: 1.6283529281616211\n",
      "Epoch 6700, Loss: 1.5438129425048828\n",
      "Epoch 6800, Loss: 1.5264246940612793\n",
      "Epoch 6900, Loss: 1.5652128219604493\n",
      "Epoch 7000, Loss: 1.426638698577881\n",
      "Epoch 7100, Loss: 1.4373178482055664\n",
      "Epoch 7200, Loss: 1.37074556350708\n",
      "Epoch 7300, Loss: 1.4697175025939941\n",
      "Epoch 7400, Loss: 1.500691032409668\n",
      "Epoch 7500, Loss: 1.6289466857910155\n",
      "Epoch 7600, Loss: 1.3565622329711915\n",
      "Epoch 7700, Loss: 1.866030502319336\n",
      "Epoch 7800, Loss: 1.6521413803100586\n",
      "Epoch 7900, Loss: 1.4492924690246582\n",
      "Epoch 8000, Loss: 1.38386812210083\n",
      "Epoch 8100, Loss: 1.5738746643066406\n",
      "Epoch 8200, Loss: 1.9678565979003906\n",
      "Epoch 8300, Loss: 1.520121955871582\n",
      "Epoch 8400, Loss: 1.6686677932739258\n",
      "Epoch 8500, Loss: 1.799898338317871\n",
      "Epoch 8600, Loss: 1.489784336090088\n",
      "Epoch 8700, Loss: 1.3638191223144531\n",
      "Epoch 8800, Loss: 1.9479286193847656\n",
      "Epoch 8900, Loss: 1.3886557579040528\n",
      "Epoch 9000, Loss: 1.8151845932006836\n",
      "Epoch 9100, Loss: 1.1815168380737304\n",
      "Epoch 9200, Loss: 1.7211841583251952\n",
      "Epoch 9300, Loss: 1.4832268714904786\n",
      "Epoch 9400, Loss: 1.7232769012451172\n",
      "Epoch 9500, Loss: 1.118354606628418\n",
      "Epoch 9600, Loss: 2.0787147521972655\n",
      "Epoch 9700, Loss: 1.6769983291625976\n",
      "Epoch 9800, Loss: 1.5879613876342773\n",
      "Epoch 9900, Loss: 1.7438520431518554\n",
      "Epoch 10000, Loss: 1.7815017700195312\n"
     ]
    }
   ],
   "source": [
    "gen = Generator()\n",
    "gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5b2c53c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"shall i compare thee to a summer's day?\\n that of grained me thou will of thy self thou contant with most stand grow,\\n and so that me thou self my self will i ergrows and for i after in the self the say,\\n and seet me for my self with for me thou pair with shadow will still stanned and heart.\\n \\n \\n \\n \\n \\n which was the will to that this light eyes for and he wert but see mind,\\n the one the beauty with new bath that under will in i not spent,\\n mine i since of that me love and thee were that heart,\\n the doth pride i not still with my self is will me strom thy doth in thee with me fill stain,\\n the se\""
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0330480b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
